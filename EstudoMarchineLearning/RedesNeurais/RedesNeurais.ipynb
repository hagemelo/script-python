{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_treino 105\n",
      "x_teste 45\n",
      "y_treino 105\n",
      "y_teste 45\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "print('x_treino %d' % len(x_treino))\n",
    "print('x_teste %d' % len(x_teste))\n",
    "print('y_treino %d' % len(y_treino))\n",
    "print('y_teste %d' % len(y_teste))\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.001\n",
    "                    , hidden_layer_sizes=(5,)\n",
    "                    , random_state=1\n",
    "                    , learning_rate='constant'\n",
    "                   , learning_rate_init=0.01\n",
    "                   ,max_iter = 100\n",
    "                   ,activation = 'logistic'\n",
    "                   ,momentum=0.9\n",
    "                   ,verbose=True\n",
    "                   ,tol = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.19215543\n",
      "Iteration 2, loss = 1.17548257\n",
      "Iteration 3, loss = 1.16009640\n",
      "Iteration 4, loss = 1.14606465\n",
      "Iteration 5, loss = 1.13340479\n",
      "Iteration 6, loss = 1.12200941\n",
      "Iteration 7, loss = 1.11161143\n",
      "Iteration 8, loss = 1.10189976\n",
      "Iteration 9, loss = 1.09263644\n",
      "Iteration 10, loss = 1.08367373\n",
      "Iteration 11, loss = 1.07493264\n",
      "Iteration 12, loss = 1.06638144\n",
      "Iteration 13, loss = 1.05801717\n",
      "Iteration 14, loss = 1.04985002\n",
      "Iteration 15, loss = 1.04189102\n",
      "Iteration 16, loss = 1.03414327\n",
      "Iteration 17, loss = 1.02659623\n",
      "Iteration 18, loss = 1.01922318\n",
      "Iteration 19, loss = 1.01198172\n",
      "Iteration 20, loss = 1.00481730\n",
      "Iteration 21, loss = 0.99766922\n",
      "Iteration 22, loss = 0.99047783\n",
      "Iteration 23, loss = 0.98319156\n",
      "Iteration 24, loss = 0.97577212\n",
      "Iteration 25, loss = 0.96819725\n",
      "Iteration 26, loss = 0.96046083\n",
      "Iteration 27, loss = 0.95257099\n",
      "Iteration 28, loss = 0.94454693\n",
      "Iteration 29, loss = 0.93641501\n",
      "Iteration 30, loss = 0.92820457\n",
      "Iteration 31, loss = 0.91994389\n",
      "Iteration 32, loss = 0.91165630\n",
      "Iteration 33, loss = 0.90335708\n",
      "Iteration 34, loss = 0.89505172\n",
      "Iteration 35, loss = 0.88673635\n",
      "Iteration 36, loss = 0.87840057\n",
      "Iteration 37, loss = 0.87003148\n",
      "Iteration 38, loss = 0.86161724\n",
      "Iteration 39, loss = 0.85314915\n",
      "Iteration 40, loss = 0.84462243\n",
      "Iteration 41, loss = 0.83603577\n",
      "Iteration 42, loss = 0.82739019\n",
      "Iteration 43, loss = 0.81868764\n",
      "Iteration 44, loss = 0.80992971\n",
      "Iteration 45, loss = 0.80111680\n",
      "Iteration 46, loss = 0.79224779\n",
      "Iteration 47, loss = 0.78332020\n",
      "Iteration 48, loss = 0.77433081\n",
      "Iteration 49, loss = 0.76527658\n",
      "Iteration 50, loss = 0.75615606\n",
      "Iteration 51, loss = 0.74697074\n",
      "Iteration 52, loss = 0.73772601\n",
      "Iteration 53, loss = 0.72843074\n",
      "Iteration 54, loss = 0.71909629\n",
      "Iteration 55, loss = 0.70973694\n",
      "Iteration 56, loss = 0.70037147\n",
      "Iteration 57, loss = 0.69102321\n",
      "Iteration 58, loss = 0.68171879\n",
      "Iteration 59, loss = 0.67248680\n",
      "Iteration 60, loss = 0.66335628\n",
      "Iteration 61, loss = 0.65435509\n",
      "Iteration 62, loss = 0.64550794\n",
      "Iteration 63, loss = 0.63683439\n",
      "Iteration 64, loss = 0.62834701\n",
      "Iteration 65, loss = 0.62005016\n",
      "Iteration 66, loss = 0.61193993\n",
      "Iteration 67, loss = 0.60400563\n",
      "Iteration 68, loss = 0.59623275\n",
      "Iteration 69, loss = 0.58860654\n",
      "Iteration 70, loss = 0.58111514\n",
      "Iteration 71, loss = 0.57375092\n",
      "Iteration 72, loss = 0.56650953\n",
      "Iteration 73, loss = 0.55938743\n",
      "Iteration 74, loss = 0.55237903\n",
      "Iteration 75, loss = 0.54547531\n",
      "Iteration 76, loss = 0.53866450\n",
      "Iteration 77, loss = 0.53193480\n",
      "Iteration 78, loss = 0.52527754\n",
      "Iteration 79, loss = 0.51868908\n",
      "Iteration 80, loss = 0.51217025\n",
      "Iteration 81, loss = 0.50572327\n",
      "Iteration 82, loss = 0.49934794\n",
      "Iteration 83, loss = 0.49303948\n",
      "Iteration 84, loss = 0.48678966\n",
      "Iteration 85, loss = 0.48059072\n",
      "Iteration 86, loss = 0.47443918\n",
      "Iteration 87, loss = 0.46833677\n",
      "Iteration 88, loss = 0.46228759\n",
      "Iteration 89, loss = 0.45629411\n",
      "Iteration 90, loss = 0.45035497\n",
      "Iteration 91, loss = 0.44446643\n",
      "Iteration 92, loss = 0.43862572\n",
      "Iteration 93, loss = 0.43283305\n",
      "Iteration 94, loss = 0.42709062\n",
      "Iteration 95, loss = 0.42139978\n",
      "Iteration 96, loss = 0.41575964\n",
      "Iteration 97, loss = 0.41016855\n",
      "Iteration 98, loss = 0.40462661\n",
      "Iteration 99, loss = 0.39913654\n",
      "Iteration 100, loss = 0.39370191\n",
      "_________________________________________X\n",
      "Saida da rede:\t [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Saida da desejada:\t [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inmetrics\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(x_treino, y_treino)\n",
    "print('_________________________________________X')\n",
    "saidas = mlp.predict(x_teste)\n",
    "print('Saida da rede:\\t', saidas)\n",
    "print('Saida da desejada:\\t', y_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(x_teste, y_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
